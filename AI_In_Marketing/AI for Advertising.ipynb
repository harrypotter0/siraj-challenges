{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo - http://www.ai-writer.com/?page=view_result&type=article&id=14300&pass=7JClv4aayH\n",
    "\n",
    "### Use cases of AI in Marketing \n",
    "\n",
    "![alt text](https://whatsthebigdata.files.wordpress.com/2017/02/ai_marketing.jpg?w=640 \"Logo Title Text 1\")\n",
    "\n",
    "- Audience Targeting (analyzing data and scoring customers based on how likely they are to take an action like purchasing a product or viewing a video) \n",
    "- Content Creation (what type of content will make a consumer most likely to convert)\n",
    "- Real-time optimization (how to optimize content in real-time to best suit a customer)\n",
    "- Machines have been automatically generating content for years. Companies such as the Associated Press, Yahoo, and Fox have been using them for quite some time.\n",
    "\n",
    "![alt text](https://blog.getresponse.com/uploads/2017/03/demandbasechart.png \"Logo Title Text 1\")\n",
    "\n",
    "### Startups in the Space\n",
    "\n",
    "#### Appier\n",
    "\n",
    "![alt text](http://www.mobyaffiliates.com/wp-content/uploads/2015/11/appier.png \"Logo Title Text 1\")\n",
    "\n",
    "- Appier is Five years old with 48.5 million dollars in total investment \n",
    "- It uses AI to predict what audience members are likely to do next. \n",
    "- It’s a real-time ad-optimization engine that finds your audience across multiple digital devices, emphasizing the mobile. \n",
    "- Its creating a more evolved and sophisticated version of your own audience data pool, which used to be called ‘cookie pools’ back in the day\n",
    "- if you know a customer just bought shoes, they’re not likely to buy another pair right away but maybe they’re looking for socks next? Enter Appier\n",
    "\n",
    "#### Drawbridge\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ai100-170927214644/95/artificial-intelligence-ai-100-startups-2017-75-638.jpg?cb=1510037429 \"Logo Title Text 1\")\n",
    "\n",
    "- Drawbridge is all about the cross-device reach and has a patent to prove it (on a “system to group internet devices based upon device usage”). \n",
    "- 45.5 million dollars invested\n",
    "- It builds an “anonymized ID” that can be used beyond advertising (for things like fraud detection)\n",
    "- They know when someone switches devices, so an advertiser can capitalize on that move\n",
    "\n",
    "####  Insidesales.com \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/il0wblpdsxgqdkrqwonw-signature-d9877cf01f81af06f005d08076d17a65d7a03928ceb1de73653a982ec29c2710-poli-170316163616/95/michael-plante-inside-sales-the-ai-revolution-6-638.jpg?cb=1490208673 \"Logo Title Text 1\")\n",
    "\n",
    "- 264 million dollars invested and founded in 2004\n",
    "- If you have a lot of prospective sales, you wont get anywhere if you try to focus on all of them at once\n",
    "- Helps predicts the 20 percent of users most likely to convert to a sale\n",
    "\n",
    "#### Persado\n",
    "\n",
    "![alt text](https://beta.techcrunch.com/wp-content/uploads/2013/02/screen-shot-2013-02-13-at-11-43-05.png \"Logo Title Text 1\")\n",
    "\n",
    "- They find the phrases and words that drive the greatest action for your audience. \n",
    "- What kind of ad will drive the greatest action for your audience? Build off of emotions and relevance.\n",
    "- They are at thetop of the funnel, driving acquisitions. \n",
    "\n",
    "### Audience Targeting\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*Zhm1NMlmVywn0G18w3exog.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://recsysjd.files.wordpress.com/2016/09/ss.png?w=368&h=226 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i.imgur.com/bmW79NS.png \"Logo Title Text 1\")\n",
    "\n",
    "- In the popular LightFM library, it generates user and item representations by functioning as a factorization machine and learning linear embeddings for each feature. \n",
    "- It takes the dot product of these two representation vectors and gets a unitless score that\n",
    "- when ranked, the score tells you how good (or bad) a given user-item match would be.\n",
    "- linear factorization method is effective and computationally efficient\n",
    "- But deep neural networks could improve on this by creating more meaningful representations \n",
    "- TensorRec is a newer library built on top of tensorflow that aims to solve that\n",
    "- TensorRec is a recommendation algorithm with an easy API for training and prediction that resembles common machine learning tools in Python. \n",
    "\n",
    "![alt text](https://www.altoros.com/blog/wp-content/uploads/2018/03/multilayer-perceptron-with-tensorflow-architecture-for-recommender-systems-v1.png \"Logo Title Text 1\")\n",
    "\n",
    "- It also gives you the flexibility to experiment with your own representation and loss functions, letting you build a recommendation system that is tailored to understanding your particular users and items.\n",
    "- It is a recommendation engine capable of learning from explicit positive and negative feedback.\n",
    "- It allows for arbitrary TensorFlow graphs to be used as representation functions and loss functions.\n",
    "- And it provides reasonable defaults for representation functions and loss functions.\n",
    "- It scores recommendations by consuming user and item features (ids, tags, or other metadata) and building two low-dimensional vectors, a “user representation” and an “item representation”. \n",
    "- The dot product of these two vectors is the score for the relationship between that user and that item — the highest scores are predicted to be the best recommendations.\n",
    "-  The algorithm used to generate these representations, called the representation function, can be customized: anything from a straight-forward linear transform to a deep neural network can be applied\n",
    "-  It learns by comparing the scores it generates to actual interactions (likes/dislikes) between users and items. \n",
    "- The comparator is called the “loss function,” and TensorRec allows you to customize your own loss functions as well.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/jfkirk/tensorrec/master/examples/system_diagram.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "#### 4 Steps!\n",
    "\n",
    "1. Transform input data into feature tensors for easy embedding.\n",
    "2. Transform user/item feature tensors into user/item representations (the representation function).\n",
    "3. Transform a pair of representations into a prediction.\n",
    "4. Transform predictions and truth values into a loss value (the loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorrec\n",
    "\n",
    "# Build the model with default parameters\n",
    "model = tensorrec.TensorRec()\n",
    "\n",
    "# Generate some dummy data\n",
    "interactions, user_features, item_features = tensorrec.util.generate_dummy_data(\n",
    "    num_users=100,\n",
    "    num_items=150,\n",
    "    interaction_density=.05\n",
    ")\n",
    "\n",
    "# Fit the model for 5 epochs\n",
    "model.fit(interactions, user_features, item_features, epochs=5, verbose=True)\n",
    "\n",
    "# Predict scores for all users and all items\n",
    "predictions = model.predict(user_features=user_features,\n",
    "                            item_features=item_features)\n",
    "\n",
    "# Calculate and print the recall at 10\n",
    "r_at_k = tensorrec.eval.recall_at_k(model, interactions,\n",
    "                                    k=10,\n",
    "                                    user_features=user_features,\n",
    "                                    item_features=item_features)\n",
    "print(np.mean(r_at_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Creation\n",
    "\n",
    "![alt text](https://www.kdnuggets.com/wp-content/uploads/RNN.jpg \"Logo Title Text 1\")\n",
    "\n",
    "http://www.ai-writer.com/?page=view_result&type=article&id=14300&pass=7JClv4aayH\n",
    "\n",
    "![alt text](http://keras.io/img/regular_stacked_lstm.png \"Logo Title Text 1\")\n",
    "\n",
    "- With AI marketers can automatically generate content for simple stories such as stock updates and sports reports. \n",
    "- You’ve probably even read content written by an algorithm without noticing it!\n",
    "- It may surprise you that the following opening sentence is a sports story written solely by an algorithm:\n",
    "\n",
    "#### “Tuesday was a great day for W. Roberts, as the junior pitcher threw a perfect game to carry Virginia to a 2-0 victory over George Washington at Davenport Field.”\n",
    "\n",
    "- Images, video? Use Generative Adversarial networks \n",
    "- Audio? Use WaveNet\n",
    "- Text? Use LSTM Recurrent Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
